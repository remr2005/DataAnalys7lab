{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание на работу: \n",
    "\n",
    "1. Спарсите спам из ваших почтовых ящиков в текстовые файлы. \n",
    "\n",
    "2. Выберите наиболее часто встречающие слова в спаме. Длина слова - не менее 5 букв. Из полученного списка удалите прилагательные (используйте модуль nltk). Выберите из оставшихся 5-7 слов – они будут индикаторами.\n",
    "\n",
    " 3. Напишите классификатор спама на основе выбранных словиндикаторов. Используйте наивный байесовский метод. \n",
    "\n",
    "4. Создайте 5 сообщений (текстов) и проверьте их на спамность вашим классификатором.\n",
    "\n",
    "5. Повторите пункты 3 и 4, применяя сглаживание в методе. При этом добавьте два слова, которые не встречались в спам-сообщениях, и в индикаторы, и в два тестовых сообщения. \n",
    "\n",
    "6. Сравните результаты и сделайте выводы. Напишите подробные комментарии в программном коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from dsmltf import tokenize, count_words, spam_probability, f1_score\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "def word_probabilities(counts:list[tuple], total_spams:int, total_non_spams:int, k=0.5) -> list[tuple]:\n",
    "    return [(w[0], (w[1] + k) / (total_spams + 2*k),\n",
    "             (w[2] + k) / (total_non_spams + 2*k))\n",
    "            for w in counts]\n",
    "\n",
    "def make_data() -> list:\n",
    "    # Загружаем необходимые ресурсы\n",
    "    nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    # Парсим данные\n",
    "    with open(\"spam.csv\") as f:\n",
    "        data = []\n",
    "        for i in csv.reader(f): data.append([i[1], 1 if i[0]==\"spam\" else 0])\n",
    "    return data\n",
    "    \n",
    "def test(words: list[tuple], dataset: list) -> float:\n",
    "    true_pos, false_pos, false_neg = 0, 0, 0\n",
    "    for i in dataset:\n",
    "        match round(spam_probability(words, i[0])),i[1]:\n",
    "            case 1,1:true_pos+=1\n",
    "            case 1,0:false_pos+=1\n",
    "            case 0,1:false_neg+=1\n",
    "    return f1_score(true_pos, false_pos, false_neg)\n",
    "\n",
    "def main() -> None:\n",
    "    dataset = make_data()\n",
    "    # количество спамных сообщений\n",
    "    spam_count = len([i for i in dataset if i[1]])\n",
    "    ham_count = len(dataset) - spam_count\n",
    "    # тренировочные данные\n",
    "    train_set = count_words(dataset[:-35])\n",
    "    # словарь с определением части речи  слова\n",
    "    tagged_keys = pos_tag(train_set.keys())\n",
    "    # отфильтрованные данные(без прилогательных)\n",
    "    train_set = defaultdict(lambda: [0, 0], {key: train_set[key] for key, tag in tagged_keys if tag not in ('JJ', 'JJR', 'JJS')})\n",
    "    # самые часто встречающиееся слова в спаме\n",
    "    words = sorted(train_set, key=lambda x: train_set[x][0] if len(x) >= 5 else 0)[-7:]\n",
    "    words = [(i, train_set[i][0]/spam_count, train_set[i][1]/ham_count if train_set[i][1]/ham_count else 0.01)  for i in words]\n",
    "    # пробуем без сглаживания\n",
    "    print(\"\", test(words, dataset[-35:]))\n",
    "    # проведем сглаживание\n",
    "    words = word_probabilities([(i[0], train_set[i[0]][0], train_set[i[0]][1]) for i in words], spam_count, ham_count)\n",
    "    print(test(words, dataset[-35:]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
